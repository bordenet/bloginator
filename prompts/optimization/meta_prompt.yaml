# Meta-prompt for LLM-driven prompt optimization
#
# This prompt guides the AI assistant (Claude) in evaluating generated content
# and determining evolutionary strategies for improving prompts across rounds.
#
# Enhanced with:
# - Explicit scoring guidance with point deductions
# - Line numbers and context for traceability
# - Voice matching with specific excerpts
# - Content quality justifications with references
# - One mutation per round (prevents confounding)
# - Hypothesis-driven approach
# - Unexpected strengths tracking
# - Suggested score adjustment field

evaluation_prompt: |
  You are an expert prompt engineer conducting an evolutionary optimization experiment.

  Your task is to:
  1. Evaluate the generated content against quality criteria
  2. Identify specific weaknesses and strengths
  3. Determine which prompt variables to mutate for the next round
  4. Provide concrete, actionable improvements

  ## EVALUATION CRITERIA

  ### AI Slop Detection (Critical)
  - Em-dashes (â€”): CRITICAL violation, auto-fail (-2.0 points each)
  - Flowery corporate jargon (e.g., "synergize", "leverage", "best-in-class"): HIGH severity (-0.5 points each)
  - Hedging words (e.g., "perhaps", "maybe", "might", "could"): MEDIUM severity (-0.25 points each)
  - Vague language (e.g., "things", "stuff", "somewhat"): LOW severity (-0.1 points each)

  Include line numbers and surrounding context for each violation to aid traceability.

  ### Voice Matching
  - Authenticity: Does it sound like the author's authentic voice from the corpus?
  - Tone consistency: Is the tone aligned with the target audience?
  - Avoidance of generic "AI voice" patterns

  Report specific excerpts showing voice mismatch and strengths reinforcing authenticity.

  ### Content Quality
  Score on a 0-5 scale with brief justification for each:
  - Clarity: Is the message clear, direct, and understandable?
  - Depth: Does the content meet the expected complexity level?
  - Nuance: Does it effectively handle subtle or complex ideas?
  - Specificity: Are concrete examples used rather than abstract platitudes?

  Provide references to relevant sections supporting each score.

  ## GENERATED CONTENT TO EVALUATE

  Test Case: {{test_case_name}}
  Title: {{title}}
  Classification: {{classification}}
  Audience: {{audience}}
  Expected Complexity: {{complexity}}/5
  Expected Nuance: {{nuance}}/5

  ### Generated Outline
  ```
  {{outline}}
  ```

  ### Generated Draft (First 3 Sections)
  ```
  {{draft_preview}}
  ```

  ## PREVIOUS ROUND RESULTS (if available)

  Round: {{round_number}}
  Previous Score: {{previous_score}}/5.0
  Previous Violations:
  - Critical: {{previous_critical}}
  - High: {{previous_high}}
  - Medium: {{previous_medium}}
  - Low: {{previous_low}}

  ## YOUR EVALUATION TASK

  Provide your evaluation in the following JSON format:

  ```json
  {
    "score": <float 0.0-5.0>,
    "slop_violations": {
      "critical": [{"pattern": "em-dash", "line": "...", "context": "..."}],
      "high": [{"pattern": "leverage synergies", "line": "...", "context": "..."}],
      "medium": [{"pattern": "perhaps", "line": "...", "context": "..."}],
      "low": [{"pattern": "things", "line": "...", "context": "..."}]
    },
    "voice_analysis": {
      "authenticity_score": <float 0.0-5.0>,
      "issues": ["sounds too corporate", "lacks personal voice", ...],
      "strengths": ["direct tone", "specific examples", ...]
    },
    "content_quality": {
      "clarity": <float 0.0-5.0>,
      "depth": <float 0.0-5.0>,
      "nuance": <float 0.0-5.0>,
      "specificity": <float 0.0-5.0>,
      "justifications": {
        "clarity": "<reference to draft>",
        "depth": "<reference to draft>",
        "nuance": "<reference to draft>",
        "specificity": "<reference to draft>"
      }
    },
    "evolutionary_strategy": {
      "prompt_to_modify": "<outline|draft|refinement>",
      "specific_changes": [
        {
          "section": "<which part of prompt>",
          "current_issue": "<what's wrong>",
          "proposed_change": "<specific modification>",
          "rationale": "<why this will help>"
        }
      ],
      "priority": "<high|medium|low>",
      "expected_impact": "<description of expected improvement>"
    },
    "priority_summary": [
      "Most critical issue to address",
      "Secondary priority"
    ],
    "unexpected_strengths": [
      "Positive patterns worth preserving",
      "Effective techniques to maintain"
    ],
    "suggested_score_adjustment": <float -5.0 to +5.0>,
    "reasoning": "<detailed analysis of what worked, what didn't, and why>"
  }
  ```

  ## EVOLUTIONARY STRATEGY GUIDELINES

  Focus on ONE primary issue per round to avoid confounding variables.
  Include exactly ONE mutation in the "specific_changes" array per round.

  Adaptive strategy based on observed violations:
  - If critical violations persist: Eliminate em-dashes and jargon
  - If voice issues dominate: Improve authenticity and tone matching
  - If content is shallow: Enhance depth and nuance
  - If too abstract: Increase specificity and concrete examples
  - If converging: Fine-tune and polish

  When modifying prompts:
  - Be SPECIFIC: State exactly what to add/remove, not just "improve clarity"
  - Be SURGICAL: Change only ONE element per round to measure impact
  - Be EVIDENCE-BASED: Reference specific violations or observed patterns
  - Be CORPUS-AWARE: Align suggestions with the author's actual voice
  - Be HYPOTHESIS-DRIVEN: Each mutation should test a measurable hypothesis

  This is a scientific experiment. Track what works and what doesn't.
