# ==============================================================================
# Bloginator Environment Configuration
# ==============================================================================
# INSTRUCTIONS:
#   1. Copy this file to .env: cp .env.example .env
#   2. Fill in your actual values (NEVER commit .env to git)
#   3. .env is already in .gitignore
# ==============================================================================

# ------------------------------------------------------------------------------
# Data Directory (Base Path)
# ------------------------------------------------------------------------------
# Base directory for all Bloginator data (corpus, chroma, output).
# Can be set to an external location outside the repo, e.g., /tmp/bloginator
# Default: .bloginator (git-ignored, in repo root)
# The directory will be created automatically if it doesn't exist (mkdir -p style)
BLOGINATOR_DATA_DIR=.bloginator

# ------------------------------------------------------------------------------
# Corpus and Storage
# ------------------------------------------------------------------------------
# These paths are relative to BLOGINATOR_DATA_DIR unless absolute paths are given.

# Directory containing your historical blog posts (markdown, PDF, docx, etc.)
BLOGINATOR_CORPUS_DIR=corpus

# ChromaDB vector store directory
BLOGINATOR_CHROMA_DIR=chroma

# Output directory for generated content
BLOGINATOR_OUTPUT_DIR=output

# ------------------------------------------------------------------------------
# LLM Provider Configuration
# ------------------------------------------------------------------------------
# NOTE: For backward compatibility, OLLAMA_HOST and OLLAMA_MODEL are also supported
# as aliases for BLOGINATOR_LLM_BASE_URL and BLOGINATOR_LLM_MODEL respectively.

# Provider: ollama, custom, openai, anthropic
BLOGINATOR_LLM_PROVIDER=ollama

# Model name
# Local Ollama models (recommended for development):
#   - mixtral:8x7b (best quality, requires ~45GB RAM, primary local model)
#   - llama3:8b (lighter alternative, requires ~8GB RAM)
# Cloud models:
#   - gpt-4-turbo, gpt-3.5-turbo (OpenAI)
#   - claude-3-opus-20240229, claude-3-sonnet-20240229 (Anthropic)
BLOGINATOR_LLM_MODEL=mixtral:8x7b

# Base URL for LLM API (used for ollama and custom providers)
# Examples:
#   - Ollama: http://localhost:11434
#   - Custom: https://your-llm-endpoint.com/v1
#   - LM Studio: http://localhost:1234/v1
BLOGINATOR_LLM_BASE_URL=http://localhost:11434

# API key for cloud LLMs (OpenAI, Anthropic, etc.)
# BLOGINATOR_LLM_API_KEY=sk-...

# ------------------------------------------------------------------------------
# Timeout Configuration (Advanced)
# ------------------------------------------------------------------------------
# All timeouts are in SECONDS. Valid range: 1 second to 86400 seconds (1 day).
# Defaults are provided in the code but can be overridden via environment variables.

# Main LLM API request timeout (default: 120 seconds)
BLOGINATOR_LLM_TIMEOUT=120

# Assistant LLM response file wait timeout (default: 300 seconds = 5 minutes)
# BLOGINATOR_ASSISTANT_LLM_RESPONSE_TIMEOUT=300

# Subprocess generation timeouts - Outline phase (default: 2700/5400/21600 seconds)
# Initial attempt timeout (default: 2700 seconds = 45 minutes)
# BLOGINATOR_SUBPROCESS_OUTLINE_TIMEOUT=2700
# Retry attempt timeout (default: 5400 seconds = 90 minutes)
# BLOGINATOR_SUBPROCESS_OUTLINE_RETRY_TIMEOUT=5400
# Final attempt timeout (default: 21600 seconds = 6 hours)
# BLOGINATOR_SUBPROCESS_OUTLINE_FINAL_TIMEOUT=21600

# Subprocess generation timeouts - Draft phase (default: 4500/7200/28800 seconds)
# Initial attempt timeout (default: 4500 seconds = 75 minutes)
# BLOGINATOR_SUBPROCESS_DRAFT_TIMEOUT=4500
# Retry attempt timeout (default: 7200 seconds = 120 minutes)
# BLOGINATOR_SUBPROCESS_DRAFT_RETRY_TIMEOUT=7200
# Final attempt timeout (default: 28800 seconds = 8 hours)
# BLOGINATOR_SUBPROCESS_DRAFT_FINAL_TIMEOUT=28800

# Model/service availability check timeouts (default: 5 seconds)
# BLOGINATOR_MODEL_AVAILABILITY_TIMEOUT=5
# BLOGINATOR_OLLAMA_TAG_CHECK_TIMEOUT=5

# Subprocess utility operation timeouts (default: 30 seconds)
# BLOGINATOR_SEARCH_SUBPROCESS_TIMEOUT=30
# BLOGINATOR_ANALYSIS_SUBPROCESS_TIMEOUT=30

# File and system operation timeouts (default: 10/15 seconds)
# BLOGINATOR_FILE_AVAILABILITY_TIMEOUT=10
# BLOGINATOR_SMB_MOUNT_TIMEOUT=15

# ------------------------------------------------------------------------------
# Custom LLM Configuration (Advanced)
# ------------------------------------------------------------------------------
# Custom headers for authentication/configuration
# Format: "Header1:Value1,Header2:Value2"
# Example: "Authorization:Bearer YOUR_TOKEN,X-Custom-Header:value"
# BLOGINATOR_LLM_CUSTOM_HEADERS=

# ------------------------------------------------------------------------------
# Generation Defaults
# ------------------------------------------------------------------------------
# Default temperature (0.0 = deterministic, 1.0 = creative)
BLOGINATOR_LLM_TEMPERATURE=0.7

# Default max tokens to generate
BLOGINATOR_LLM_MAX_TOKENS=2000

# ------------------------------------------------------------------------------
# Web UI (Optional)
# ------------------------------------------------------------------------------
# Host for web interface
BLOGINATOR_WEB_HOST=0.0.0.0

# Port for web interface
BLOGINATOR_WEB_PORT=8000

# ------------------------------------------------------------------------------
# Debug
# ------------------------------------------------------------------------------
# Enable debug logging
BLOGINATOR_DEBUG=false

# ==============================================================================
# Example Configurations
# ==============================================================================

# Example 1: Ollama (Local LLM)
# BLOGINATOR_LLM_PROVIDER=ollama
# BLOGINATOR_LLM_MODEL=llama3
# BLOGINATOR_LLM_BASE_URL=http://localhost:11434
#
# Example 1b: Ollama on Network Server
# BLOGINATOR_LLM_PROVIDER=ollama
# BLOGINATOR_LLM_MODEL=llama3:8b
# BLOGINATOR_LLM_BASE_URL=http://192.168.5.53:11434  # Find your IP: ipconfig getifaddr en0

# Example 2: LM Studio (Local LLM)
# BLOGINATOR_LLM_PROVIDER=custom
# BLOGINATOR_LLM_MODEL=local-model
# BLOGINATOR_LLM_BASE_URL=http://localhost:1234/v1

# Example 3: Custom Remote Endpoint
# BLOGINATOR_LLM_PROVIDER=custom
# BLOGINATOR_LLM_MODEL=your-model-name
# BLOGINATOR_LLM_BASE_URL=https://your-endpoint.com/api
# BLOGINATOR_LLM_CUSTOM_HEADERS="Authorization:Bearer YOUR_TOKEN"

# Example 4: OpenAI
# BLOGINATOR_LLM_PROVIDER=openai
# BLOGINATOR_LLM_MODEL=gpt-4-turbo
# BLOGINATOR_LLM_API_KEY=sk-...

# Example 5: Anthropic
# BLOGINATOR_LLM_PROVIDER=anthropic
# BLOGINATOR_LLM_MODEL=claude-3-opus-20240229
# BLOGINATOR_LLM_API_KEY=sk-ant-...
